# OVDEval
<h3 align="center"> A Comprehensive Evaluation Benchmark for Open-Vocabulary Detection</h3>
<p align="center">
 <a href="https://arxiv.org/abs/2308.13177"><strong> [Paper ðŸ“„] </strong></a>
</p>


***
OVDEval is a new benchmark for OVD model, which includes 9 sub-tasks and introduces evaluations on commonsense 
knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset is 
meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input.
Additionally, we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these 
fine-grained label datasets and propose a new metric called Non-Maximum Suppression Average Precision (NMS-AP) to address this issue.

***
<h3 align="center"> Benchmark </h3>


***
<img src="docs/example.png" alt="OVDEval" width="100%">
